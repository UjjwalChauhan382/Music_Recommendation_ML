{"cells":[{"cell_type":"code","execution_count":6,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":false,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-09-10T06:15:02.825138Z","iopub.status.busy":"2023-09-10T06:15:02.824768Z","iopub.status.idle":"2023-09-10T06:15:02.831759Z","shell.execute_reply":"2023-09-10T06:15:02.829286Z","shell.execute_reply.started":"2023-09-10T06:15:02.825106Z"},"trusted":true},"outputs":[],"source":["from keras.models import Sequential\n","from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Flatten\n","from keras.optimizers import Adam\n","from keras.preprocessing.image import ImageDataGenerator"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-09-10T06:16:17.536715Z","iopub.status.busy":"2023-09-10T06:16:17.535981Z","iopub.status.idle":"2023-09-10T06:16:17.541626Z","shell.execute_reply":"2023-09-10T06:16:17.540615Z","shell.execute_reply.started":"2023-09-10T06:16:17.536678Z"},"trusted":true},"outputs":[],"source":["train_data_gen = ImageDataGenerator(rescale = 1./255)\n","validation_data_gen = ImageDataGenerator(rescale = 1./255)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-09-10T06:18:32.509792Z","iopub.status.busy":"2023-09-10T06:18:32.509406Z","iopub.status.idle":"2023-09-10T06:18:36.634531Z","shell.execute_reply":"2023-09-10T06:18:36.633506Z","shell.execute_reply.started":"2023-09-10T06:18:32.509763Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 28709 images belonging to 7 classes.\n"]}],"source":["train_generator = train_data_gen.flow_from_directory('/kaggle/input/emotion-detection-fer/train', target_size=(48, 48), batch_size=64, color_mode='grayscale', class_mode='categorical')"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-09-10T06:24:53.633950Z","iopub.status.busy":"2023-09-10T06:24:53.633565Z","iopub.status.idle":"2023-09-10T06:24:54.217657Z","shell.execute_reply":"2023-09-10T06:24:54.216634Z","shell.execute_reply.started":"2023-09-10T06:24:53.633919Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 7178 images belonging to 7 classes.\n"]}],"source":["validation_generator = validation_data_gen.flow_from_directory('/kaggle/input/emotion-detection-fer/test',target_size=(48,48),batch_size=64, color_mode='grayscale',class_mode='categorical')"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-09-10T06:25:26.516703Z","iopub.status.busy":"2023-09-10T06:25:26.516292Z","iopub.status.idle":"2023-09-10T06:25:26.525177Z","shell.execute_reply":"2023-09-10T06:25:26.524137Z","shell.execute_reply.started":"2023-09-10T06:25:26.516669Z"},"trusted":true},"outputs":[],"source":["emotion_model = Sequential()"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-09-10T06:32:41.525979Z","iopub.status.busy":"2023-09-10T06:32:41.525598Z","iopub.status.idle":"2023-09-10T06:32:41.738521Z","shell.execute_reply":"2023-09-10T06:32:41.736322Z","shell.execute_reply.started":"2023-09-10T06:32:41.525946Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super().__init__(name, **kwargs)\n"]}],"source":["emotion_model.add(Conv2D(32, kernel_size=(3,3),activation='relu',input_shape=(48,48,1)))\n","emotion_model.add(Conv2D(64,kernel_size=(3,3),activation='relu'))\n","emotion_model.add(MaxPooling2D(pool_size=(2,2)))\n","emotion_model.add(Dropout(0.25))\n","\n","emotion_model.add(Conv2D(128,kernel_size=(3,3),activation='relu'))\n","emotion_model.add(MaxPooling2D(pool_size=(2,2)))\n","emotion_model.add(Conv2D(128,kernel_size=(3,3),activation='relu'))\n","emotion_model.add(MaxPooling2D(pool_size=(2,2)))\n","emotion_model.add(Dropout(0.25))\n","\n","emotion_model.add(Flatten())\n","emotion_model.add(Dense(1024, activation='relu'))\n","emotion_model.add(Dropout(0.5))\n","emotion_model.add(Dense(7,activation='relu'))\n","\n","emotion_model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.0001, decay=1e-6), metrics=['accuracy'])"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-09-10T06:37:20.753744Z","iopub.status.busy":"2023-09-10T06:37:20.753011Z","iopub.status.idle":"2023-09-10T07:10:01.672659Z","shell.execute_reply":"2023-09-10T07:10:01.671519Z","shell.execute_reply.started":"2023-09-10T06:37:20.753709Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_28/1514010088.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n","  emotion_model_info=emotion_model.fit_generator(train_generator, steps_per_epoch=28709//64, epochs=50, validation_data=validation_generator, validation_steps=7178//64)\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/50\n"]},{"name":"stderr","output_type":"stream","text":["2023-09-10 06:37:22.083411: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_1/dropout/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"]},{"name":"stdout","output_type":"stream","text":["448/448 [==============================] - 156s 328ms/step - loss: 1.9984 - accuracy: 0.2399 - val_loss: 1.8184 - val_accuracy: 0.2472\n","Epoch 2/50\n","448/448 [==============================] - 33s 74ms/step - loss: 1.8529 - accuracy: 0.2510 - val_loss: 1.8246 - val_accuracy: 0.2473\n","Epoch 3/50\n","448/448 [==============================] - 34s 75ms/step - loss: 1.8447 - accuracy: 0.2508 - val_loss: 1.8251 - val_accuracy: 0.2472\n","Epoch 4/50\n","448/448 [==============================] - 38s 85ms/step - loss: 1.8275 - accuracy: 0.2515 - val_loss: 1.8017 - val_accuracy: 0.2515\n","Epoch 5/50\n","448/448 [==============================] - 33s 73ms/step - loss: 1.8259 - accuracy: 0.2574 - val_loss: 1.8015 - val_accuracy: 0.2586\n","Epoch 6/50\n","448/448 [==============================] - 34s 76ms/step - loss: 1.7978 - accuracy: 0.2705 - val_loss: 1.7825 - val_accuracy: 0.3055\n","Epoch 7/50\n","448/448 [==============================] - 37s 83ms/step - loss: 1.7861 - accuracy: 0.2908 - val_loss: 1.7437 - val_accuracy: 0.3066\n","Epoch 8/50\n","448/448 [==============================] - 33s 74ms/step - loss: 1.7580 - accuracy: 0.3072 - val_loss: 1.7445 - val_accuracy: 0.3252\n","Epoch 9/50\n","448/448 [==============================] - 34s 77ms/step - loss: 1.7431 - accuracy: 0.3145 - val_loss: 1.6927 - val_accuracy: 0.3304\n","Epoch 10/50\n","448/448 [==============================] - 33s 73ms/step - loss: 1.7227 - accuracy: 0.3268 - val_loss: 1.6692 - val_accuracy: 0.3493\n","Epoch 11/50\n","448/448 [==============================] - 34s 75ms/step - loss: 1.7218 - accuracy: 0.3497 - val_loss: 1.6653 - val_accuracy: 0.3810\n","Epoch 12/50\n","448/448 [==============================] - 33s 73ms/step - loss: 1.7266 - accuracy: 0.3453 - val_loss: 1.6241 - val_accuracy: 0.3728\n","Epoch 13/50\n","448/448 [==============================] - 32s 72ms/step - loss: 1.6730 - accuracy: 0.3636 - val_loss: 1.6337 - val_accuracy: 0.3471\n","Epoch 14/50\n","448/448 [==============================] - 33s 73ms/step - loss: 1.6593 - accuracy: 0.3740 - val_loss: 1.6078 - val_accuracy: 0.3984\n","Epoch 15/50\n","448/448 [==============================] - 32s 71ms/step - loss: 1.6688 - accuracy: 0.3654 - val_loss: 1.6234 - val_accuracy: 0.3698\n","Epoch 16/50\n","448/448 [==============================] - 33s 75ms/step - loss: 1.6413 - accuracy: 0.3794 - val_loss: 1.5797 - val_accuracy: 0.4065\n","Epoch 17/50\n","448/448 [==============================] - 32s 70ms/step - loss: 1.6268 - accuracy: 0.3893 - val_loss: 1.5731 - val_accuracy: 0.4065\n","Epoch 18/50\n","448/448 [==============================] - 37s 82ms/step - loss: 1.6217 - accuracy: 0.3972 - val_loss: 1.5517 - val_accuracy: 0.4164\n","Epoch 19/50\n","448/448 [==============================] - 37s 82ms/step - loss: 1.5889 - accuracy: 0.4133 - val_loss: 1.5346 - val_accuracy: 0.4282\n","Epoch 20/50\n","448/448 [==============================] - 31s 70ms/step - loss: 1.6232 - accuracy: 0.4002 - val_loss: 1.6325 - val_accuracy: 0.3703\n","Epoch 21/50\n","448/448 [==============================] - 37s 83ms/step - loss: 1.5831 - accuracy: 0.4091 - val_loss: 1.5047 - val_accuracy: 0.4360\n","Epoch 22/50\n","448/448 [==============================] - 32s 71ms/step - loss: 1.6103 - accuracy: 0.3979 - val_loss: 1.5868 - val_accuracy: 0.4043\n","Epoch 23/50\n","448/448 [==============================] - 37s 83ms/step - loss: 1.5806 - accuracy: 0.4135 - val_loss: 1.5194 - val_accuracy: 0.4374\n","Epoch 24/50\n","448/448 [==============================] - 33s 73ms/step - loss: 1.5712 - accuracy: 0.4264 - val_loss: 1.6106 - val_accuracy: 0.3622\n","Epoch 25/50\n","448/448 [==============================] - 32s 72ms/step - loss: 1.5588 - accuracy: 0.4206 - val_loss: 1.5194 - val_accuracy: 0.4337\n","Epoch 26/50\n","448/448 [==============================] - 33s 74ms/step - loss: 1.5275 - accuracy: 0.4365 - val_loss: 1.4746 - val_accuracy: 0.4551\n","Epoch 27/50\n","448/448 [==============================] - 32s 72ms/step - loss: 1.5797 - accuracy: 0.4066 - val_loss: 1.6331 - val_accuracy: 0.3701\n","Epoch 28/50\n","448/448 [==============================] - 33s 74ms/step - loss: 1.5810 - accuracy: 0.4108 - val_loss: 1.4821 - val_accuracy: 0.4435\n","Epoch 29/50\n","448/448 [==============================] - 33s 74ms/step - loss: 1.5055 - accuracy: 0.4408 - val_loss: 1.4359 - val_accuracy: 0.4568\n","Epoch 30/50\n","448/448 [==============================] - 31s 70ms/step - loss: 1.5001 - accuracy: 0.4483 - val_loss: 1.4629 - val_accuracy: 0.4651\n","Epoch 31/50\n","448/448 [==============================] - 33s 73ms/step - loss: 1.5066 - accuracy: 0.4437 - val_loss: 1.4286 - val_accuracy: 0.4573\n","Epoch 32/50\n","448/448 [==============================] - 32s 72ms/step - loss: 1.4786 - accuracy: 0.4624 - val_loss: 1.4739 - val_accuracy: 0.4577\n","Epoch 33/50\n","448/448 [==============================] - 32s 72ms/step - loss: 1.4909 - accuracy: 0.4501 - val_loss: 1.4713 - val_accuracy: 0.4764\n","Epoch 34/50\n","448/448 [==============================] - 39s 87ms/step - loss: 1.4471 - accuracy: 0.4711 - val_loss: 1.4507 - val_accuracy: 0.4784\n","Epoch 35/50\n","448/448 [==============================] - 33s 73ms/step - loss: 1.5186 - accuracy: 0.4320 - val_loss: 1.9054 - val_accuracy: 0.2660\n","Epoch 36/50\n","448/448 [==============================] - 34s 76ms/step - loss: 1.7338 - accuracy: 0.3322 - val_loss: 1.6342 - val_accuracy: 0.3955\n","Epoch 37/50\n","448/448 [==============================] - 34s 76ms/step - loss: 1.6421 - accuracy: 0.3906 - val_loss: 1.5479 - val_accuracy: 0.4240\n","Epoch 38/50\n","448/448 [==============================] - 38s 84ms/step - loss: 1.5794 - accuracy: 0.4223 - val_loss: 1.4822 - val_accuracy: 0.4475\n","Epoch 39/50\n","448/448 [==============================] - 34s 75ms/step - loss: 1.5156 - accuracy: 0.4400 - val_loss: 1.4379 - val_accuracy: 0.4562\n","Epoch 40/50\n","448/448 [==============================] - 32s 72ms/step - loss: 1.4926 - accuracy: 0.4495 - val_loss: 1.4505 - val_accuracy: 0.4688\n","Epoch 41/50\n","448/448 [==============================] - 34s 75ms/step - loss: 1.5095 - accuracy: 0.4429 - val_loss: 1.4789 - val_accuracy: 0.4443\n","Epoch 42/50\n","448/448 [==============================] - 37s 82ms/step - loss: 1.4911 - accuracy: 0.4541 - val_loss: 1.4321 - val_accuracy: 0.4768\n","Epoch 43/50\n","448/448 [==============================] - 33s 74ms/step - loss: 1.5858 - accuracy: 0.4052 - val_loss: 1.5543 - val_accuracy: 0.4277\n","Epoch 44/50\n","448/448 [==============================] - 37s 84ms/step - loss: 1.5920 - accuracy: 0.4013 - val_loss: 1.4869 - val_accuracy: 0.4556\n","Epoch 45/50\n","448/448 [==============================] - 34s 76ms/step - loss: 1.5306 - accuracy: 0.4338 - val_loss: 1.4544 - val_accuracy: 0.4614\n","Epoch 46/50\n","448/448 [==============================] - 34s 76ms/step - loss: 1.5282 - accuracy: 0.4312 - val_loss: 1.4556 - val_accuracy: 0.4630\n","Epoch 47/50\n","448/448 [==============================] - 34s 75ms/step - loss: 1.5177 - accuracy: 0.4363 - val_loss: 1.4283 - val_accuracy: 0.4647\n","Epoch 48/50\n","448/448 [==============================] - 34s 76ms/step - loss: 1.6578 - accuracy: 0.3815 - val_loss: 1.6603 - val_accuracy: 0.3485\n","Epoch 49/50\n","448/448 [==============================] - 33s 74ms/step - loss: 1.6689 - accuracy: 0.3692 - val_loss: 1.5626 - val_accuracy: 0.4329\n","Epoch 50/50\n","448/448 [==============================] - 33s 73ms/step - loss: 1.5915 - accuracy: 0.4104 - val_loss: 1.4963 - val_accuracy: 0.4407\n"]}],"source":["emotion_model_info=emotion_model.fit_generator(train_generator, steps_per_epoch=28709//64, epochs=50, validation_data=validation_generator, validation_steps=7178//64)"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-09-10T07:11:35.623678Z","iopub.status.busy":"2023-09-10T07:11:35.623280Z","iopub.status.idle":"2023-09-10T07:11:35.684544Z","shell.execute_reply":"2023-09-10T07:11:35.683613Z","shell.execute_reply.started":"2023-09-10T07:11:35.623647Z"},"trusted":true},"outputs":[{"ename":"NameError","evalue":"name 'emotion_model' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32mc:\\Users\\Shakshi\\Downloads\\emotion-detection.ipynb Cell 8\u001b[0m line \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Shakshi/Downloads/emotion-detection.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model_json \u001b[39m=\u001b[39m emotion_model\u001b[39m.\u001b[39mto_json()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Shakshi/Downloads/emotion-detection.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39memotion_model.json\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mw\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m json_file:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Shakshi/Downloads/emotion-detection.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     json_file\u001b[39m.\u001b[39mwrite(model_json)\n","\u001b[1;31mNameError\u001b[0m: name 'emotion_model' is not defined"]}],"source":["model_json = emotion_model.to_json()\n","with open(\"emotion_model.json\", \"w\") as json_file:\n","    json_file.write(model_json)\n","\n","emotion_model.save_weights(\"emotion_model.h5\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"}},"nbformat":4,"nbformat_minor":4}
